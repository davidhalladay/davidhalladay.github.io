---
layout: default
title: Text-to-Image
parent: Paper-Bookmark
nav_order: 1
---


## Text to Image

### General

| V    | Model                                                        | Paper                                                        | Note                                                         |
| ---- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
|      | [Paper](https://arxiv.org/pdf/1511.02793.pdf)                | [ICLR2016] GENERATING IMAGES FROM CAPTIONS WITH ATTENTION    |                                                              |
| V    | [StackGAN](https://arxiv.org/abs/1612.03242)<br />[Code](https://github.com/hanzhanggit/StackGAN) | [ICCV2017]StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks |                                                              |
|      | [I2T2I](https://arxiv.org/pdf/1703.06676.pdf)                | [2017] I2T2I: LEARNING TEXT TO IMAGE SYNTHESIS WITH TEXTUAL DATA AUGMENTATION |                                                              |
|      | [LR-GAN](https://arxiv.org/pdf/1703.01560.pdf)               | [ICLR2017] LR-GAN: LAYERED RECURSIVE GENERATIVE ADVERSARIAL NETWORKS FOR IMAGE GENERATION |                                                              |
|      | [CanvasGAN](https://arxiv.org/pdf/1810.02833.pdf)            | [2018] CanvasGAN: A simple baseline for text to image generation by incrementally patching a canvas | We propose a new recurrent generative model for generating images from text captions while attending on specific parts of text captions. |
| V    | [StackGAN++](https://arxiv.org/abs/1710.10916)<br />[Code](https://github.com/hanzhanggit/StackGAN-v2) | [TPAMI2018]StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks |                                                              |
| V    | [AttnGAN](https://arxiv.org/abs/1711.10485)<br />[Code](https://github.com/taoxugit/AttnGAN) | [CVPR2018]AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks |                                                              |
|      | [HD-GAN](https://arxiv.org/pdf/1802.09178.pdf)<br />[Code](https://github.com/ypxie/HDGan) | [CVPR2018]Photographic Text-to-Image Synthesis with a Hierarchically-nested Adversarial Network |                                                              |
|      | [Paper](https://zpascal.net/cvpr2018/Hong_Inferring_Semantic_Layout_CVPR_2018_paper.pdf) | [CVPR2018]Inferring Semantic Layout for Hierarchical Text-to-Image Synthesis |                                                              |
|      | [GAN-INT-CLS](https://arxiv.org/abs/1605.05396)              | [WSDM2018]Mining Knowledge Graphs from Text                  |                                                              |
|      | [StoryGAN](https://arxiv.org/abs/1812.02784)<br />[Code](https://github.com/yitong91/StoryGAN) | [CVPR2019]StoryGAN: A Sequential Conditional GAN for Story Visualization |                                                              |
| V    | [MirrorGAN](https://arxiv.org/abs/1903.05854)                | [CVPR2019]MirrorGAN: Learning Text-to-image Generation by Redescription |                                                              |
|      | [DM-GAN](https://arxiv.org/abs/1904.01310)                   | [CVPR2019] DM-GAN: Dynamic Memory Generative Adversarial Networks for Text-to-Image Synthesis |                                                              |
|      | [SD-GAN](https://arxiv.org/abs/1904.01480)                   | [CVPR2019] Semantics Disentangling for Text-to-Image Generation |                                                              |
| V    | [Obj-GAN](https://arxiv.org/abs/1902.10740)<br />[Code](https://github.com/jamesli1618/Obj-GAN) | [CVPR2019] Object-driven Text-to-Image Synthesis via Adversarial Training |                                                              |
| V    | [Paper](https://arxiv.org/pdf/1811.09845.pdf)                | [ICCV2019] Tell, Draw, and Repeat: Generating and Modifying Images Based on Continual Linguistic Instruction |                                                              |
| V    | [Paper](https://arxiv.org/pdf/1908.01741.pdf)                | [2019] Visual-Relation Conscious Image Generation from Structured-Text | ![07](./images/07.png)                                       |
|      | [Paper](https://arxiv.org/pdf/1909.00640.pdf)                | [2019] Relationship-Aware Spatial Perception Fusion for Realistic Scene Layout Generation |                                                              |
|      | [Paper](https://arxiv.org/pdf/1910.13321v1.pdf)              | [2019] Semantic Object Accuracy for Generative Text-to-Image Synthesis | a new model that explicitly models individual objects within an image and a new evaluation metric called Semantic Object Accuracy (SOA) |
|      | [Paper](https://arxiv.org/pdf/1805.08318.pdf)                | [ICML2019] Self-Attention Generative Adversarial Networks    |                                                              |

### Bbox constrained
| V    | Model                                                        | Paper                                                        | Note |
| ---- | ------------------------------------------------------------ | ------------------------------------------------------------ | ---- |
| V    | [GAWWN](https://arxiv.org/abs/1610.02454)<br />[Code](https://github.com/reedscot/nips2016) | [NIPS2016]Learning What and Where to Draw                    |      |
| V    | [Object Pathways](https://arxiv.org/abs/1901.00686)<br />[Code](https://github.com/tohinz/multiple-objects-gan) | [ICLR2019]Generating Multiple Objects at Spatially Distinct Locations |      |